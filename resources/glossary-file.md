# LLM & AI Glossary

## A

**Agent** - An AI system that can autonomously pursue goals by reasoning, using tools, and taking actions.

**Attention Mechanism** - The core mechanism in transformers that allows models to focus on relevant parts of input.

**API (Application Programming Interface)** - Interface for programmatic access to LLM services.

## B

**BLEU Score** - Metric for evaluating text generation quality by comparing to reference texts.

**Base Model** - Pre-trained model before fine-tuning or instruction training.

## C

**Chain-of-Thought (CoT)** - Prompting technique where the model explains its reasoning step-by-step.

**Chunking** - Splitting documents into smaller pieces for RAG systems.

**Context Window** - Maximum amount of text (in tokens) a model can process at once.

**Constitutional AI** - Training approach that builds in safety principles and ethical guidelines.

## D

**Decoder** - Component of transformer that generates output text.

**Dense Retrieval** - Using embeddings for semantic search (vs keyword search).

## E

**Embedding** - Numerical vector representation of text that captures semantic meaning.

**Encoder** - Component of transformer that processes input text.

**Evaluation** - Measuring model performance on specific tasks.

## F

**Few-Shot Learning** - Providing examples in the prompt to guide model behavior.

**Fine-Tuning** - Further training a pre-trained model on specific data.

**Function Calling** - Model's ability to call external functions/tools in structured format.

## G

**Generative AI** - AI systems that create new content (text, images, code, etc.).

**Grounding** - Connecting model outputs to verifiable sources or facts.

## H

**Hallucination** - When models generate false or nonsensical information confidently.

**RLHF (Reinforcement Learning from Human Feedback)** - Training technique using human preferences.

## I

**In-Context Learning** - Model learning from examples provided in the prompt.

**Inference** - Running a trained model to generate predictions/outputs.

**Instruction Tuning** - Training models to follow instructions better.

## J

**Jailbreaking** - Attempts to bypass model safety guidelines.

## K

**K-Shot Learning** - Providing K examples in the prompt (K=0 is zero-shot, K=1 is one-shot, etc.).

**Knowledge Base** - Structured collection of information for RAG systems.

## L

**Latency** - Time delay between request and response.

**LLM (Large Language Model)** - AI model trained on vast text data to understand and generate language.

**LoRA (Low-Rank Adaptation)** - Efficient fine-tuning technique.

## M

**MCP (Model Context Protocol)** - Standard protocol for connecting LLMs to data sources and tools.

**Multi-Agent System** - Multiple AI agents working together.

**Multi-Modal** - Models that work with multiple types of data (text, images, audio).

## N

**NLP (Natural Language Processing)** - Field of AI focused on language understanding.

**Neural Network** - Computing system inspired by biological neural networks.

## O

**Orchestration** - Coordinating multiple LLM calls and tools to accomplish tasks.

**Output Token** - Each word/subword generated by the model (usually costs more than input tokens).

## P

**Parameters** - Learnable weights in a neural network (7B = 7 billion parameters).

**Perplexity** - Metric measuring how well a model predicts text.

**Prompt** - Input text given to a language model.

**Prompt Engineering** - Crafting effective prompts to get desired outputs.

**Prompt Injection** - Malicious input designed to manipulate model behavior.

## Q

**Quantization** - Reducing model precision to save memory (32-bit → 8-bit → 4-bit).

**Query** - User question or search term.

## R

**RAG (Retrieval-Augmented Generation)** - Combining retrieval of relevant documents with text generation.

**ReAct** - Agent pattern combining Reasoning and Acting.

**Reranking** - Re-ordering retrieved documents by relevance.

**ROUGE Score** - Metric for evaluating text summarization.

## S

**Semantic Search** - Finding information based on meaning, not just keywords.

**Similarity Search** - Finding similar items using vector embeddings.

**System Prompt** - Initial instructions that set model behavior.

## T

**Temperature** - Parameter controlling randomness in generation (0=deterministic, 1+=creative).

**Token** - Basic unit of text for language models (roughly ¾ of a word in English).

**Token Limit** - Maximum tokens allowed in context window.

**Tool Use** - Model's ability to call external functions/APIs.

**Transformer** - Neural network architecture underlying modern LLMs.

## U

**User Prompt** - The actual query or instruction from the user.

## V

**Vector** - Numerical array representing text in multi-dimensional space.

**Vector Database** - Database optimized for storing and searching vectors.

**Vectorization** - Converting text to vector embeddings.

## W

**Weight** - Numerical parameter in neural network.

## Z

**Zero-Shot Learning** - Model performing task without examples in prompt.

---

## Common Acronyms

- **AI** - Artificial Intelligence
- **API** - Application Programming Interface
- **CoT** - Chain-of-Thought
- **DPO** - Direct Preference Optimization
- **GPT** - Generative Pre-trained Transformer
- **LLM** - Large Language Model
- **MCP** - Model Context Protocol
- **ML** - Machine Learning
- **NLP** - Natural Language Processing
- **RAG** - Retrieval-Augmented Generation
- **RLHF** - Reinforcement Learning from Human Feedback
- **SFT** - Supervised Fine-Tuning

---

## Model Names Explained

- **GPT-3.5** - OpenAI's 3.5 generation model
- **GPT-4** - OpenAI's most capable model
- **Claude 3 Opus** - Anthropic's largest Claude 3 model
- **Claude 3 Sonnet** - Anthropic's balanced model
- **Llama 2 70B** - Meta's 70 billion parameter model
- **Mistral 7B** - Mistral AI's 7 billion parameter model

---

## Common Metrics

- **BLEU** - Bilingual Evaluation Understudy (translation quality)
- **ROUGE** - Recall-Oriented Understudy for Gisting Evaluation (summarization)
- **Perplexity** - How "surprised" model is by text (lower = better)
- **F1 Score** - Balance of precision and recall
- **Accuracy** - Percentage of correct predictions

---

*Last updated: October 2025*